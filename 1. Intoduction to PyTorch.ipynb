{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ShkcD1s51-"
      },
      "source": [
        "# **PyTorch Overview**\n",
        "\n",
        "## Open-Source Deep Learning Library\n",
        "\n",
        "Developed by **Meta AI** (formerly Facebook AI Research).\n",
        "\n",
        "---\n",
        "\n",
        "## Python & Torch\n",
        "\n",
        "- Combines Python’s ease of use with the efficiency of the **Torch** scientific computing framework, originally built with **Lua**.  \n",
        "- Torch was known for high-performance tensor-based operations, especially on **GPUs**.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6iePanOs5ye"
      },
      "source": [
        "## Core Features:\n",
        "\n",
        "1. **Tensor Computations**  \n",
        "2. **GPU Acceleration**  \n",
        "3. **Dynamic Computation Graph**  \n",
        "4. **Automatic Differentiation**  \n",
        "5. **Distributed Training**  \n",
        "6. **Interoperability with other libraries**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVjPvYFhs5wQ"
      },
      "source": [
        "\n",
        "\n",
        "# **PyTorch vs TensorFlow Comparison**\n",
        "\n",
        "| **Aspect**               | **PyTorch**                                  | **TensorFlow**                                        | **Verdict**                                |\n",
        "| ------------------------ | -------------------------------------------- | ----------------------------------------------------- | ------------------------------------------ |\n",
        "| **Language**             | Pythonic interface, deeply integrated        | Multi-language (Python, C++, JS, etc.)                | Depends: PyTorch for Python; TF for multi. |\n",
        "| **Ease of Use**          | Intuitive and beginner-friendly              | Improved with TF 2.x, but still complex               | ✅ PyTorch Wins                             |\n",
        "| **Deployment**           | TorchScript, PyTorch Mobile; improving tools | TF Serving, Lite, JS – mature tools                   | ✅ TensorFlow Wins                          |\n",
        "| **Performance**          | Competitive, dynamic graphs + JIT            | Static graphs + XLA – highly optimized                | ⚖️ Tie                                     |\n",
        "| **Community**            | Strong in academia; fast-growing ecosystem   | Large industrial adoption; extensive tools            | Depends: PyTorch (research); TF (industry) |\n",
        "| **Mobile & Embedded**    | PyTorch Mobile, quantization support         | TF Lite & TF.js – robust and mature                   | ✅ TensorFlow Wins                          |\n",
        "| **Use Case Focus**       | Best for research, prototyping, CV/NLP       | Broad industry applications                           | Depends on context                         |\n",
        "| **Learning Curve**       | Easier due to dynamic, Pythonic design       | Steep, but better in TF 2.x                           | ✅ PyTorch Wins                             |\n",
        "| **Interoperability**     | Python-native; exports to ONNX               | TF Hub, SavedModel, partial ONNX support              | ✅ PyTorch Wins                             |\n",
        "| **Customizability**      | Highly flexible and easy to modify           | Possible, but more complex                            | ✅ PyTorch Wins                             |\n",
        "| **Deployment Tools**     | TorchServe, cloud integrations               | TF Serving, TFX – complete ML pipeline support        | ✅ TensorFlow Wins                          |\n",
        "| **Distributed Training** | `torch.distributed`, Horovod                 | `tf.distribute.Strategy` – advanced and user-friendly | ✅ TensorFlow Wins                          |\n",
        "| **Pre-trained Models**   | TorchVision, HuggingFace                     | TF Hub, Keras Zoo                                     | ⚖️ Tie                                     |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kwmEbXs5uf"
      },
      "source": [
        "# **Core PyTorch Modules**\n",
        "\n",
        "| **Module**                | **Description**                                                                                                                                           |\n",
        "|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `torch`                  | The core module providing multidimensional arrays (tensors) and mathematical operations on them.                                                          |\n",
        "| `torch.autograd`         | Automatic differentiation engine that records operations on tensors to compute gradients for optimization.                                               |\n",
        "| `torch.nn`               | Provides a neural networks library, including layers, activations, loss functions, and utilities to build deep learning models.                          |\n",
        "| `torch.optim`            | Contains optimization algorithms (optimizers) like SGD, Adam, and RMSprop used for training neural networks.                                              |\n",
        "| `torch.utils.data`       | Utilities for data handling, including the `Dataset` and `DataLoader` classes for managing and loading datasets efficiently.                              |\n",
        "| `torch.jit`              | Supports Just-In-Time (JIT) compilation and TorchScript for optimizing models and enabling deployment without Python dependencies.                        |\n",
        "| `torch.distributed`      | Tools for distributed training across multiple GPUs and machines, facilitating parallel computation.                                                      |\n",
        "| `torch.cuda`             | Interfaces with NVIDIA CUDA to enable GPU acceleration for tensor computations and model training.                                                        |\n",
        "| `torch.backends`         | Contains settings and allows control over backend libraries like cuDNN, MKL, and others for performance tuning.                                           |\n",
        "| `torch.multiprocessing`  | Utilities for parallelism using multiprocessing, similar to Python's `multiprocessing` module but with support for CUDA tensors.                          |\n",
        "| `torch.quantization`     | Tools for model quantization to reduce model size and improve inference speed, especially on edge devices.                                                |\n",
        "| `torch.onnx`             | Supports exporting PyTorch models to the ONNX (Open Neural Network Exchange) format for interoperability with other frameworks and deployment.           |\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO3OMqNPs5ph"
      },
      "source": [
        "# **PyTorch Domain Libraries**\n",
        "\n",
        "| **Library**         | **Description**                                                                                                                              |\n",
        "| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `torchvision`       | Provides datasets, model architectures, and image transformations for computer vision tasks.                                                 |\n",
        "| `torchtext`         | Tools and datasets for natural language processing (NLP), including data preprocessing and vocabulary management.                            |\n",
        "| `torchaudio`        | Utilities for audio processing tasks, including I/O, transforms, and pre-trained models for speech recognition.                              |\n",
        "| `torcharrow`        | A library for accelerated data loading and preprocessing, especially for tabular and time series data (experimental).                        |\n",
        "| `torchserve`        | A PyTorch model serving library that makes it easy to deploy trained models at scale in production environments.                             |\n",
        "| `pytorch_lightning` | A lightweight wrapper for PyTorch that simplifies the training loop and reduces boilerplate code, enabling scalable and reproducible models. |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2i2Ouo7s5mI"
      },
      "source": [
        "# **Popular PyTorch Ecosystem Libraries**\n",
        "\n",
        "| **Library**                   | **Description**                                                                                                                             |\n",
        "| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Hugging Face Transformers** | Provides state-of-the-art pre-trained models for NLP tasks like text classification, translation, and question answering, built on PyTorch. |\n",
        "| **Fastai**                    | High-level library that simplifies training fast and accurate neural nets using modern best practices, built on top of PyTorch.             |\n",
        "| **PyTorch Geometric**         | Extension library for geometric deep learning, including graph neural networks and 3D data processing.                                      |\n",
        "| **TorchMetrics**              | A modular metrics API for PyTorch, compatible with PyTorch Lightning and provides standardized implementations of many common metrics.      |\n",
        "| **TorchElastic**              | Enables dynamic scaling of PyTorch distributed training jobs, allowing for elasticity in resource management.                               |\n",
        "| **Optuna**                    | An automatic hyperparameter optimization software framework, integrating well with PyTorch for tuning models.                               |\n",
        "| **Catalyst**                  | Provides high-level features for training neural networks, focusing on reproducibility and fast experimentation.                            |\n",
        "| **Ignite**                    | High-level library to help with training neural networks in PyTorch, offering a lightweight engine for training and evaluating models.      |\n",
        "| **AllenNLP**                  | An NLP research library built on PyTorch, designed to support researchers in deep learning for NLP.                                         |\n",
        "| **Skorch**                    | A scikit-learn compatible wrapper for PyTorch that allows the use of PyTorch models with scikit-learn utilities and APIs.                   |\n",
        "| **PyTorch Forecasting**       | High-level library for time series forecasting, making it easy to build, train, and evaluate complex models.                                |\n",
        "| **TensorBoard for PyTorch**   | Allows visualization of training metrics, model graphs, and other useful data within TensorBoard for PyTorch models.                        |\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
